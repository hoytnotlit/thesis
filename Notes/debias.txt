NOTEs 29.4
Discuss the reliability of the collected biases, consider maybe using a questionnaire to gather data tms to replicate the experiments
Write about masked language modeling
1.5.
Write about benefits of more context in predicting masked word - maybe results would be more realistic if the sentences have a hint on what the word could be.
--
ADD: "In summary, the working definition of bias is ...."

Explain more about language models/BERT (how its trained) to non-computational linguists/reader

(Dropout regularization - write about it?)

Explain association score, what do the numbers mean etc, formulas+numbered steps and refer to numbers in text

Fix latex tables (resizebox) p()

arxiv references are not peer reviewed, if possible use real paper (use google scholar to find all references)

Next meeting 4.5. 10:30

- get gold labels/ association scores from another model (multilingual)
- run self diagnosis/debias



-- DROPOUT REGULARIZATION --
For Wikipedia, the recommended pre-processing is to download the latest dump, extract the text with WikiExtractor.py, and then apply any necessary cleanup to convert it into plain text.

dumps.wikipedia -> get data 
wikiextractor -> process data

-> get/create txt file
-> sklearn to split data
-> download finnish bert (from transformers) locally on server
-> modify the config (increase dropout values, like in that one paper)
-> train 

https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling

run in deb-env:
python ~/transformers/examples/pytorch/language-modeling/run_mlm.py     --model_name_or_path ~/bert-base-finnish-cased-v1     --train_file ~/thesis/Wiki/wikidata_train.txt     --validation_file ~/thesis/Wiki/wikidata_test.txt     --per_device_train_batch_size 8     --per_device_eval_batch_size 8     --do_train     --do_eval     --output_dir tmp/test-mlm     --save_total_limit=3  --max_seq_length=256


BERT has two dropout parameters which
may be configured, one for attention weights (a)
and another for hidden activations (h), both set to
.10 by default. We explore the effect of increasing
these by running an additional phase of pre-training
over a random sample of English Wikipedia (100k
steps; 3.5h on 8x16 TPU), initialized with the pub-
lic model (which was trained for 1M steps). Table 4
shows the best results (lowest correlation metrics)
seen for a grid search over the values .10, .15 and
.20, for a = .15 and h = .20
https://arxiv.org/pdf/2010.06032.pdf

OLD
-> run through create_pretraining_data.py
-> run_pretraining.py




Virtual environments:
venv for running tests
deb-venv for training/debiasing model
TODO specify the transformers versions in thesis?